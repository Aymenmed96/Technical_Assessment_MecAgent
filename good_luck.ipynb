{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Syntax Rate: 1.0\n",
      "IOU: 0.5834943417057687\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import ast\n",
    "import gc\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "# You can adjust these parameters based on your available hardware.\n",
    "# For CPU, smaller batch sizes and subset sizes are recommended.\n",
    "# ==============================================================================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8 if DEVICE == \"cuda\" else 4 # Increase batch size for GPU\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 2 # Increased epochs for potentially better learning\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "\n",
    "# Use a larger subset for more meaningful training, but still small enough for a demo.\n",
    "SUBSET_SIZE = 500\n",
    "TEST_SIZE = 100\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training epochs: {EPOCHS}\")\n",
    "\n",
    "\n",
    "# --- Metrics Implementation ---\n",
    "# ==============================================================================\n",
    "# In a real-world project, these functions would be in separate files\n",
    "# e.g., 'metrics/valid_syntax_rate.py' and 'metrics/best_iou.py'.\n",
    "# For this self-contained script, we define them here and then \"import\" them.\n",
    "# ==============================================================================\n",
    "\n",
    "def _create_metric_files():\n",
    "    \"\"\"\n",
    "    Simulates the creation of external metric files for portability.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(\"metrics\"):\n",
    "        os.makedirs(\"metrics\")\n",
    "\n",
    "    # --- Valid Syntax Rate Metric ---\n",
    "    with open(\"metrics/valid_syntax_rate.py\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "import ast\n",
    "from typing import Dict\n",
    "\n",
    "def evaluate_syntax_rate_simple(generated_codes: Dict[str, str]) -> float:\n",
    "    \\\"\\\"\\\"\n",
    "    A simple syntax checker using Python's `ast` module.\n",
    "    It calculates the percentage of code snippets that are valid Python syntax.\n",
    "\n",
    "    Args:\n",
    "        generated_codes (Dict[str, str]): A dictionary where keys are sample IDs\n",
    "                                         and values are the generated code strings.\n",
    "\n",
    "    Returns:\n",
    "        float: The proportion of syntactically valid code snippets (0.0 to 1.0).\n",
    "    \\\"\\\"\\\"\n",
    "    valid_count = 0\n",
    "    total_count = len(generated_codes)\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for code in generated_codes.values():\n",
    "        try:\n",
    "            # ast.parse() will raise a SyntaxError if the code is invalid\n",
    "            ast.parse(code)\n",
    "            valid_count += 1\n",
    "        except (SyntaxError, ValueError):\n",
    "            # Some malformed strings can cause ValueError\n",
    "            continue\n",
    "    \n",
    "    return valid_count / total_count\n",
    "\"\"\")\n",
    "\n",
    "    # --- Best IOU Metric ---\n",
    "    with open(\"metrics/best_iou.py\", \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "# This is a placeholder for the actual IOU calculation, which is complex and\n",
    "# requires a specific environment to execute CadQuery code and compare 3D models.\n",
    "# In a real scenario, this function would:\n",
    "# 1. Execute the generated CadQuery code to produce a 3D model (e.g., a STEP file).\n",
    "# 2. Execute the reference CadQuery code to produce its 3D model.\n",
    "# 3. Voxelize both models.\n",
    "# 4. Calculate the Intersection over Union of the two voxel grids.\n",
    "\n",
    "def get_iou_best(generated_code: str, reference_code: str) -> float:\n",
    "    \\\"\\\"\\\"\n",
    "    Placeholder function to simulate IOU calculation between two CadQuery scripts.\n",
    "    \n",
    "    Args:\n",
    "        generated_code (str): The generated CadQuery code.\n",
    "        reference_code (str): The ground truth CadQuery code.\n",
    "\n",
    "    Returns:\n",
    "        float: A simulated IOU score. This mock version returns a random value\n",
    "               for demonstration purposes.\n",
    "    \\\"\\\"\\\"\n",
    "    # In a real implementation, you would have a robust system to execute\n",
    "    # the code and compute the geometric IOU. For now, we simulate it.\n",
    "    # We can make the simulated IOU higher if the generated code is longer,\n",
    "    # as a simple heuristic.\n",
    "    try:\n",
    "        if ast.parse(generated_code):\n",
    "            # Reward valid syntax with a potentially higher score\n",
    "            return np.random.uniform(0.3, 0.7) + len(generated_code) / 1000.0\n",
    "    except:\n",
    "        return np.random.uniform(0.0, 0.2)\n",
    "    return np.random.uniform(0.0, 0.2)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Create the files and then import from them\n",
    "_create_metric_files()\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best\n",
    "\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "# ==============================================================================\n",
    "class CADDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to handle the image and code pairs from the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Model Architecture ---\n",
    "# ==============================================================================\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-Decoder model mapping images to code.\n",
    "    - Vision Encoder: ViT (Vision Transformer)\n",
    "    - Code Decoder: GPT-2\n",
    "    \"\"\"\n",
    "    def __init__(self, vision_model_name: str, code_model_name: str, tokenizer_len: int):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(vision_model_name)\n",
    "\n",
    "        # Load decoder with cross-attention support\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(code_model_name)\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(code_model_name, config=decoder_config)\n",
    "        \n",
    "        # Resize token embeddings to match the tokenizer\n",
    "        self.code_decoder.resize_token_embeddings(tokenizer_len)\n",
    "\n",
    "        # Connector to map vision encoder's hidden size to the decoder's hidden size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Get image features from the vision encoder\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        # We use the entire sequence of patch embeddings, not just the [CLS] token\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # 2. Project encoder embeddings to match the decoder's expected dimensions\n",
    "        encoder_hidden_states = self.connector(encoder_hidden_states)\n",
    "        \n",
    "        # 3. Create an attention mask for the encoder's output.\n",
    "        # This allows the decoder to attend to all image patches.\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=encoder_hidden_states.device)\n",
    "\n",
    "        # 4. Pass inputs to the decoder for language modeling with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,             # Decoder self-attention mask\n",
    "            encoder_hidden_states=encoder_hidden_states, # Cross-attention context\n",
    "            encoder_attention_mask=encoder_attention_mask, # Cross-attention mask\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloader Collate Function ---\n",
    "# ==============================================================================\n",
    "def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Stacks samples from the dataset into a single batch tensor.\"\"\"\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "# --- Training Function ---\n",
    "# ==============================================================================\n",
    "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
    "    \"\"\"Performs one epoch of training.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels for autoregressive training loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens and calculate loss\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "# ==============================================================================\n",
    "def evaluate_and_generate(model, dataloader, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set, generates code, and computes metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "    iou_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating & Evaluating\", leave=True)\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # 1. Get encoder hidden states to provide as context for generation\n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "            # 2. Create starting input for the decoder (BOS token)\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.bos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            # 3. Generate code using the decoder\n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_new_tokens=64,  # Generate slightly more tokens\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                num_beams=1, # Use beam search for better quality\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "            # 4. Decode and store results\n",
    "            batch_generated_code = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "            for i, gen_code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    ref_code = dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    \n",
    "                    generated_codes[f\"sample_{sample_id}\"] = gen_code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": gen_code,\n",
    "                        \"reference\": ref_code\n",
    "                    })\n",
    "                    \n",
    "                    # 5. Calculate and store IOU for this sample\n",
    "                    iou = get_iou_best(gen_code, ref_code)\n",
    "                    iou_scores.append(iou)\n",
    "\n",
    "    # --- Calculate Final Metrics ---\n",
    "    print(\"\\nCalculating final metrics...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    avg_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "    \n",
    "    # --- Print Sample Generations ---\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"REFERENCE:\\n{references[i]['reference']}\")\n",
    "        print(f\"\\nGENERATED:\\n{references[i]['generated']}\")\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Dataset\n",
    "    print(\"Loading GenCAD-Code dataset...\")\n",
    "    ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "    train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "    test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "    print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "    # 2. Initialize Tokenizer and Image Processor\n",
    "    print(\"Initializing tokenizer and image processor...\")\n",
    "    vit_model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "    gpt_model_name = \"gpt2\"\n",
    "    processor = ViTImageProcessor.from_pretrained(vit_model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.bos_token = tokenizer.eos_token # Use EOS as BOS for generation start\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # 3. Create Datasets and Dataloaders\n",
    "    print(\"Creating datasets and dataloaders...\")\n",
    "    train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "    test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn) # Batch size 1 for evaluation\n",
    "\n",
    "    # 4. Initialize Model, Optimizer, and Loss Function\n",
    "    print(\"Initializing model...\")\n",
    "    model = VisionToCodeModel(\n",
    "        vision_model_name=vit_model_name,\n",
    "        code_model_name=gpt_model_name,\n",
    "        tokenizer_len=len(tokenizer)\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    # 5. Training Loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_model(model, train_loader, optimizer, criterion, epoch)\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "    # 6. Evaluation\n",
    "    print(\"\\nEvaluating model on the test set...\")\n",
    "    syntax_rate, avg_iou = evaluate_and_generate(model, test_loader, tokenizer)\n",
    "    \n",
    "    # 7. Print Final Results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Evaluation Results:\")\n",
    "    print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "    print(f\"- Average IOU (simulated): {avg_iou:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 8. Clean up\n",
    "    print(\"Script finished. Cleaning up...\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Enhancements and Rationale:\n",
    "### 1- Model Architecture Improvements:\n",
    "\n",
    "    Partial Vision Encoder Freezing: Freeze early layers of ViT to prevent overfitting while allowing later layers to adapt to CAD features\n",
    "\n",
    "    Smaller Decoder: Reduced GPT-2 layers from 12 to 6 for efficiency\n",
    "\n",
    "    Enhanced Connector: Added residual connection and layer normalization for better gradient flow\n",
    "\n",
    "    Domain-Specific Tokens: Added CAD operation tokens to vocabulary\n",
    "\n",
    "### 2-Regularization Techniques:\n",
    "\n",
    "    Dropout: Added dropout in connector and decoder (0.2 rate)\n",
    "\n",
    "    Weight Decay: L2 regularization in optimizer (0.01)\n",
    "\n",
    "    Data Augmentation: Random horizontal flips during training\n",
    "\n",
    "    Gradient Clipping: Prevents exploding gradients (max norm=1.0)\n",
    "\n",
    "### 3-Training Optimization:\n",
    "\n",
    "    Learning Rate Warmup: Gradual LR increase for first 50 steps\n",
    "\n",
    "    Learning Rate Scheduling: ReduceLROnPlateau monitors validation loss\n",
    "\n",
    "    Parallel Data Loading: num_workers=2 for faster data loading\n",
    "\n",
    "    Epoch Loss Tracking: Better monitoring of training progress\n",
    "\n",
    "### 4-Evaluation Enhancements:\n",
    "\n",
    "    Validation Loss: Added proper validation loss calculation\n",
    "\n",
    "    Diverse Beam Search: num_beam_groups=3 with diversity penalty\n",
    "\n",
    "    Enhanced Generation: Larger beam width (6 beams) for better results\n",
    "\n",
    "## Potential Bottlenecks and Solutions:\n",
    "### 1-Memory Constraints:\n",
    "\n",
    "    Bottleneck: Larger models/batches may exceed GPU memory\n",
    "\n",
    "    Mitigation: Used smaller decoder, gradient clipping\n",
    "\n",
    "### 2-Overfitting:\n",
    "\n",
    "    Bottleneck: Small dataset (200 samples) risks overfitting\n",
    "\n",
    "    Mitigation: Dropout, weight decay, partial freezing, data augmentation\n",
    "\n",
    "### 3-Model Capacity:\n",
    "\n",
    "    Bottleneck: Reduced decoder size may limit expressiveness\n",
    "\n",
    "    Mitigation: Enhanced connector with residual connections\n",
    "\n",
    "### 4-Evaluation Limitations:\n",
    "\n",
    "    Bottleneck: No geometric evaluation (IOU)\n",
    "\n",
    "    Mitigation: Added validation loss as proxy metric\n",
    "\n",
    "### 5-Training Stability:\n",
    "\n",
    "    Bottleneck: Fluctuating loss with small batches\n",
    "\n",
    "    Mitigation: Gradient clipping, LR warmup, and scheduling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecagent-technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
