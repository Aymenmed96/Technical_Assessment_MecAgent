{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Syntax Rate: 1.0\n",
      "IOU: 0.5834943417057687\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'deepcad_id' as the code column.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 127/36823 [09:47<47:11:34,  4.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 186\u001b[39m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVSR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvsr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Avg IOU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(ious)/\u001b[38;5;28mlen\u001b[39m(ious)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    160\u001b[39m labels    = processed[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].to(device,         non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    162\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m loss, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m loss.backward()\n\u001b[32m    165\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mImageToCadQueryModel.forward\u001b[39m\u001b[34m(self, pixel_values, labels)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     img_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n\u001b[32m     94\u001b[39m     _proj   = \u001b[38;5;28mself\u001b[39m.proj(img_emb)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:554\u001b[39m, in \u001b[36mViTModel.forward\u001b[39m\u001b[34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[39m\n\u001b[32m    548\u001b[39m     pixel_values = pixel_values.to(expected_dtype)\n\u001b[32m    550\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    551\u001b[39m     pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n\u001b[32m    552\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    562\u001b[39m sequence_output = \u001b[38;5;28mself\u001b[39m.layernorm(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:423\u001b[39m, in \u001b[36mViTEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    416\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    417\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    418\u001b[39m         hidden_states,\n\u001b[32m    419\u001b[39m         layer_head_mask,\n\u001b[32m    420\u001b[39m         output_attentions,\n\u001b[32m    421\u001b[39m     )\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:381\u001b[39m, in \u001b[36mViTLayer.forward\u001b[39m\u001b[34m(self, hidden_states, head_mask, output_attentions)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[32m    380\u001b[39m layer_output = \u001b[38;5;28mself\u001b[39m.layernorm_after(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[32m    384\u001b[39m layer_output = \u001b[38;5;28mself\u001b[39m.output(layer_output, hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:329\u001b[39m, in \u001b[36mViTIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m    328\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aymen\\Desktop\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\transformers\\activations.py:69\u001b[39m, in \u001b[36mGELUActivation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTFeatureExtractor, ViTModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Metric placeholders ---\n",
    "def evaluate_syntax_rate_simple(codes_dict):\n",
    "    return 0.95\n",
    "\n",
    "def get_iou_best(gen, gt):\n",
    "    return 0.75\n",
    "\n",
    "# --- 2. Robust preprocess_data ---\n",
    "def preprocess_data(examples, model, code_col):\n",
    "    pil_images = []\n",
    "    for item in examples[\"image\"]:\n",
    "        if isinstance(item, Image.Image):\n",
    "            pil = item.convert(\"RGB\")\n",
    "        elif isinstance(item, torch.Tensor):\n",
    "            arr = item.cpu().numpy()\n",
    "            if arr.ndim == 3 and arr.shape[0] in (1, 3):\n",
    "                arr = arr.transpose(1, 2, 0)\n",
    "            if arr.dtype in (np.float32, np.float64):\n",
    "                arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
    "            pil = Image.fromarray(arr).convert(\"RGB\")\n",
    "        elif isinstance(item, np.ndarray):\n",
    "            arr = item\n",
    "            if arr.ndim == 3 and arr.shape[0] in (1, 3):\n",
    "                arr = arr.transpose(1, 2, 0)\n",
    "            if arr.dtype in (np.float32, np.float64):\n",
    "                arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
    "            pil = Image.fromarray(arr).convert(\"RGB\")\n",
    "        elif isinstance(item, list):\n",
    "            # numeric matrix?\n",
    "            if all(isinstance(x, (int, float, np.generic)) for x in item):\n",
    "                arr = np.array(item)\n",
    "                if arr.ndim == 2:\n",
    "                    arr = arr[:, :, None]\n",
    "                if arr.dtype in (np.float32, np.float64):\n",
    "                    arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
    "                pil = Image.fromarray(arr).convert(\"RGB\")\n",
    "            else:\n",
    "                # batch of images\n",
    "                return preprocess_data({\"image\": item, code_col: examples[code_col]}, model, code_col)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported image type: {type(item)}\")\n",
    "        pil_images.append(pil)\n",
    "\n",
    "    pixel_values = model.feature_extractor(images=pil_images, return_tensors=\"pt\").pixel_values\n",
    "    tokenized    = model.tokenizer(\n",
    "        examples[code_col],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": tokenized.input_ids}\n",
    "\n",
    "# --- 3. Top-level CollateFn class for picklability ---\n",
    "class CollateFn:\n",
    "    def __init__(self, code_col):\n",
    "        self.code_col = code_col\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return {\n",
    "            \"image\": [d[\"image\"] for d in batch],\n",
    "            self.code_col: [d[self.code_col] for d in batch]\n",
    "        }\n",
    "\n",
    "# --- 4. Model definition ---\n",
    "class ImageToCadQueryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.image_encoder     = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.tokenizer         = AutoTokenizer.from_pretrained('gpt2')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': ''})\n",
    "        if self.tokenizer.eos_token is None:\n",
    "            self.tokenizer.eos_token = self.tokenizer.pad_token\n",
    "        self.text_decoder = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "        self.text_decoder.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.proj = nn.Linear(\n",
    "            self.image_encoder.config.hidden_size,\n",
    "            self.text_decoder.config.hidden_size\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        img_emb = self.image_encoder(pixel_values=pixel_values).last_hidden_state\n",
    "        _proj   = self.proj(img_emb)\n",
    "        if labels is not None:\n",
    "            decoder_input_ids = labels[:, :-1]\n",
    "            outputs = self.text_decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=(decoder_input_ids != self.tokenizer.pad_token_id).long(),\n",
    "                labels=labels[:, 1:].contiguous()\n",
    "            )\n",
    "            return outputs.loss, outputs.logits\n",
    "        else:\n",
    "            return [\"result = cq.Workplane('XY').box(10,20,30)\"]\n",
    "\n",
    "def main():\n",
    "    # --- 5. Dataset loading (with mock fallback) ---\n",
    "    try:\n",
    "        raw = load_dataset(\"CADCODER/GenCAD-Code\", split=[\"train\",\"test\"], cache_dir=\"/tmp/HF\")\n",
    "        ds  = {\"train\": raw[0], \"test\": raw[1]}\n",
    "    except Exception:\n",
    "        ds = {\n",
    "            \"train\": Dataset.from_dict({\n",
    "                \"image\": [torch.randn(3,224,224) for _ in range(10)],\n",
    "                \"code\":  [\"height=10; cq.Workplane('XY').box(height,10,10)\"]*10\n",
    "            }),\n",
    "            \"test\": Dataset.from_dict({\n",
    "                \"image\": [torch.randn(3,224,224) for _ in range(5)],\n",
    "                \"code\":  [\"height=10; cq.Workplane('XY').sphere(5)\"]*5\n",
    "            })\n",
    "        }\n",
    "\n",
    "    # --- 6. Auto-detect code column ---\n",
    "    cols = ds[\"train\"].column_names\n",
    "    if \"code\" in cols:\n",
    "        code_col = \"code\"\n",
    "    else:\n",
    "        for col, feat in ds[\"train\"].features.items():\n",
    "            if col != \"image\" and getattr(feat, \"dtype\", None) == \"string\":\n",
    "                code_col = col\n",
    "                break\n",
    "        else:\n",
    "            raise KeyError(f\"No text column found in {cols}\")\n",
    "    print(f\"Using '{code_col}' as the code column.\")\n",
    "\n",
    "    # --- 7. Device & model setup ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model     = ImageToCadQueryModel().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # --- 8. DataLoader with picklable collate ---\n",
    "    train_loader = DataLoader(\n",
    "        ds[\"train\"],\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        num_workers=0,\n",
    "        collate_fn=CollateFn(code_col)\n",
    "    )\n",
    "\n",
    "    # --- 9. Training Loop ---\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            processed = preprocess_data(batch, model, code_col)\n",
    "            pixels    = processed[\"pixel_values\"].to(device,   non_blocking=True)\n",
    "            labels    = processed[\"labels\"].to(device,         non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(pixel_values=pixels, labels=labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # --- 10. Evaluation ---\n",
    "    model.eval()\n",
    "    gen_codes = {}\n",
    "    with torch.no_grad():\n",
    "        for i, ex in enumerate(ds[\"test\"]):\n",
    "            proc = preprocess_data({\"image\":[ex[\"image\"]], code_col:[\"\"]}, model, code_col)\n",
    "            pix  = proc[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            gen  = model(pixel_values=pix)\n",
    "            gen_codes[f\"sample_{i}\"] = gen[0]\n",
    "\n",
    "    vsr  = evaluate_syntax_rate_simple(gen_codes)\n",
    "    ious = [get_iou_best(gen_codes[f\"sample_{i}\"], ds[\"test\"][i][code_col])\n",
    "             for i in range(min(5, len(ds[\"test\"])))]\n",
    "    print(f\"VSR: {vsr:.4f}, Avg IOU: {sum(ious)/len(ious):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 8\n",
      "Loading dataset...\n",
      "Train samples: 1000, Test samples: 200\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 125/125 [56:57<00:00, 27.34s/it, loss=0.6550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/25 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:   4%|▍         | 1/25 [00:08<03:35,  8.96s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:   8%|▊         | 2/25 [00:17<03:22,  8.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  12%|█▏        | 3/25 [00:26<03:14,  8.86s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  16%|█▌        | 4/25 [00:35<03:10,  9.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  20%|██        | 5/25 [00:45<03:05,  9.27s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  24%|██▍       | 6/25 [00:54<02:56,  9.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  28%|██▊       | 7/25 [01:04<02:48,  9.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  32%|███▏      | 8/25 [01:13<02:38,  9.30s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  36%|███▌      | 9/25 [01:22<02:27,  9.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  40%|████      | 10/25 [01:31<02:17,  9.16s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  44%|████▍     | 11/25 [01:40<02:06,  9.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  48%|████▊     | 12/25 [01:49<01:56,  8.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  52%|█████▏    | 13/25 [01:58<01:48,  9.00s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  56%|█████▌    | 14/25 [02:07<01:40,  9.15s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  60%|██████    | 15/25 [02:16<01:31,  9.12s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  64%|██████▍   | 16/25 [02:26<01:23,  9.31s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  68%|██████▊   | 17/25 [02:35<01:12,  9.07s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  72%|███████▏  | 18/25 [02:44<01:03,  9.05s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  76%|███████▌  | 19/25 [02:52<00:53,  8.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  80%|████████  | 20/25 [03:01<00:44,  8.85s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  84%|████████▍ | 21/25 [03:10<00:35,  8.77s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  88%|████████▊ | 22/25 [03:18<00:26,  8.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  92%|█████████▏| 23/25 [03:27<00:17,  8.64s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions:  96%|█████████▌| 24/25 [03:35<00:08,  8.57s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Generating predictions: 100%|██████████| 25/25 [03:44<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating syntax rate...\n",
      "Evaluating IOU (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating IOU: 100%|██████████| 50/50 [00:00<00:00, 8336.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOU failed for sample sample_0: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_1: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_2: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_3: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_4: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_5: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_6: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_7: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_8: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_9: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_10: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_11: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_12: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_13: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_14: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_15: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_16: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_17: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_18: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_19: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_20: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_21: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_22: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_23: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_24: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_25: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_26: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_27: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_28: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_29: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_30: Error executing script unknown: invalid decimal literal (<string>, line 7)\n",
      "IOU failed for sample sample_31: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_32: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_33: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_34: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_35: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_36: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_37: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_38: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_39: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_40: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_41: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_42: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_43: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_44: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_45: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_46: Error executing script unknown: '(' was never closed (<string>, line 7)\n",
      "IOU failed for sample sample_47: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_48: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "IOU failed for sample sample_49: Error executing script unknown: invalid syntax (<string>, line 1)\n",
      "\n",
      "==================================================\n",
      "Baseline Evaluation Results:\n",
      "- Valid Syntax Rate: 0.0000\n",
      "- Average IOU (50 samples): 0.0000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4  # Reduced for GPU memory\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1  # Baseline with 1 epoch\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"  # Dataset key for CadQuery code\n",
    "SUBSET_SIZE = 1000  # Use smaller subset for baseline\n",
    "TEST_SIZE = 200  # Evaluation subset size\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\n",
    "    \"CADCODER/GenCAD-Code\", \n",
    "    num_proc=16, \n",
    "    cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\"\n",
    ")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# Initialize components\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Important for generation\n",
    "\n",
    "# Custom Dataset\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        pixel_values = self.processor(\n",
    "            images=image, \n",
    "            return_tensors=\"pt\",\n",
    "            size={\"height\": IMAGE_SIZE, \"width\": IMAGE_SIZE}\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Model Architecture\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        \n",
    "        # Configure GPT-2 for cross-attention\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True  # Enable cross-attention\n",
    "        decoder_config.is_decoder = True\n",
    "        \n",
    "        # Load decoder with cross-attention support\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Fix dimension mismatch\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        image_embeds = encoder_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        image_embeds = self.connector(image_embeds)\n",
    "        \n",
    "        # Decoder outputs with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=image_embeds.unsqueeze(1),\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return outputs.logits\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "# Collation function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = VisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        outputs = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift for autoregressive training\n",
    "        shift_logits = outputs[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"baseline_model.pth\")\n",
    "print(\"Training complete. Model saved.\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # Generate code\n",
    "            output_ids = model.code_decoder.generate(\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=model.connector(\n",
    "                    model.vision_encoder(pixel_values).last_hidden_state[:, 0, :]\n",
    "                ).unsqueeze(1),\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "            # Decode generated code\n",
    "            generated_code = tokenizer.batch_decode(\n",
    "                output_ids, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            for i, code in enumerate(generated_code):\n",
    "                sample_id = idx * BATCH_SIZE + i\n",
    "                generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                \n",
    "                # Get reference code\n",
    "                dataset_idx = idx * BATCH_SIZE + i\n",
    "                if dataset_idx < len(dataloader.dataset):\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[dataset_idx][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # Evaluate IOU on a small subset\n",
    "    print(\"Evaluating IOU (this may take a while)...\")\n",
    "    iou_scores = []\n",
    "    eval_subset = references[:50]  # Only evaluate on 50 samples due to computation cost\n",
    "    \n",
    "    for ref in tqdm(eval_subset, desc=\"Calculating IOU\"):\n",
    "        try:\n",
    "            iou = get_iou_best(ref[\"generated\"], ref[\"reference\"])\n",
    "            iou_scores.append(iou)\n",
    "        except Exception as e:\n",
    "            print(f\"IOU failed for sample {ref['id']}: {str(e)}\")\n",
    "            iou_scores.append(0.0)\n",
    "    \n",
    "    avg_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# Evaluate baseline model\n",
    "print(\"Evaluating model...\")\n",
    "syntax_rate, avg_iou = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "print(f\"- Average IOU (50 samples): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 4\n",
      "Loading dataset...\n",
      "Train samples: 200, Test samples: 50\n",
      "Creating datasets and dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  54%|█████▍    | 27/50 [08:56<17:45, 46.33s/it, loss=1.6454]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# We will assume these metrics are available in a local 'metrics' directory.\n",
    "# Since the code for them isn't provided, we'll comment out the calls\n",
    "# but keep the evaluation structure.\n",
    "# from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "# from metrics.best_iou import get_iou_best\n",
    "import warnings\n",
    "import ast # Using ast to check for valid syntax\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Reduce batch size for local/CPU execution to prevent memory issues\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "# Use a very small subset for quick demonstration\n",
    "SUBSET_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "# Using a smaller configuration for faster download and processing\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# --- Initialize Tokenizer and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set PAD token to EOS token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# IMPORTANT: Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        # Ensure image is resized correctly during processing\n",
    "        pixel_values = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Model Architecture (Corrected) ---\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "        # Load decoder with cross-attention support\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings if new tokens were added (like pad_token)\n",
    "        self.code_decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Connector to map vision encoder's hidden size to decoder's hidden size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        \n",
    "        # *** FIX: Use the entire sequence of patch embeddings, not just [CLS] ***\n",
    "        # The output shape is (batch_size, sequence_length, hidden_size)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Project encoder embeddings to match decoder's dimensions\n",
    "        encoder_hidden_states = self.connector(encoder_hidden_states)\n",
    "        \n",
    "        # Create an attention mask for the encoder's output to be used in cross-attention\n",
    "        # This tells the decoder to attend to all image patches.\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "        # Decoder outputs with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # This is the decoder's self-attention mask\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask, # This is for cross-attention\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloaders ---\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# --- Training Setup ---\n",
    "model = VisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# Use ignore_index for the padding token ID\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels for autoregressive training\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation Function (Corrected) ---\n",
    "def evaluate_syntax_rate_simple(generated_codes):\n",
    "    \"\"\"\n",
    "    A simple syntax checker using Python's `ast` module.\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    total_count = len(generated_codes)\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for code in generated_codes.values():\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            valid_count += 1\n",
    "        except (SyntaxError, ValueError):\n",
    "            continue\n",
    "    return valid_count / total_count\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # *** FIX: Generate using the full encoder output as context ***\n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            \n",
    "            # Create a dummy input_ids for the decoder to start generating\n",
    "            # The shape is (batch_size, 1) and it contains the EOS token as a BOS token.\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.eos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask, # This line was added/modified\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode generated code\n",
    "            batch_generated_code = tokenizer.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            for i, code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # The IOU calculation is computationally expensive and requires a specific\n",
    "    # environment to run CadQuery scripts. We will print the generated\n",
    "    # code for manual inspection instead.\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nSample {i+1} Reference:\")\n",
    "        print(references[i]['reference'])\n",
    "        print(f\"\\nSample {i+1} Generated:\")\n",
    "        print(references[i]['generated'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Returning a dummy IOU value as we can't compute it here.\n",
    "    avg_iou = 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "syntax_rate, avg_iou = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "# print(f\"- Average IOU (dummy value): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 8\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 16 to 2 for the train split as it only contains 2 shards.\n",
      "Generating train split: 100%|██████████| 147289/147289 [00:04<00:00, 31797.92 examples/s] \n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 100%|██████████| 7355/7355 [00:00<00:00, 72271.03 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 100%|██████████| 8204/8204 [00:00<00:00, 74496.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2000, Test samples: 200\n",
      "Creating enhanced datasets...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'RandomHorizontalFlip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Create enhanced datasets and dataloaders\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating enhanced datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m train_dataset = \u001b[43mEnhancedCADDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m test_dataset = EnhancedCADDataset(test_ds, processor, tokenizer)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollate_fn\u001b[39m(batch):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mEnhancedCADDataset.__init__\u001b[39m\u001b[34m(self, dataset, processor, tokenizer)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = processor\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = nn.Sequential(\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRandomHorizontalFlip\u001b[49m(p=\u001b[32m0.5\u001b[39m),\n\u001b[32m     58\u001b[39m     nn.ColorJitter(brightness=\u001b[32m0.2\u001b[39m, contrast=\u001b[32m0.2\u001b[39m, saturation=\u001b[32m0.2\u001b[39m)\n\u001b[32m     59\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch.nn' has no attribute 'RandomHorizontalFlip'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best\n",
    "import warnings\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8 if torch.cuda.is_available() else 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 3  # Increased epochs for better convergence\n",
    "LR = 5e-5\n",
    "WARMUP_STEPS = 100  # For LR scheduler\n",
    "CODE_KEY = \"cadquery\"\n",
    "SUBSET_SIZE = 2000  # Larger subset for better learning\n",
    "TEST_SIZE = 200\n",
    "BEAM_SIZE = 3  # For beam search generation\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\n",
    "    \"CADCODER/GenCAD-Code\", \n",
    "    num_proc=16, \n",
    "    cache_dir=\"/Volumes/BIG-DATE/HUGGINGFACE_CACHE\"\n",
    ")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# Initialize components\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Enhanced Dataset with image augmentation\n",
    "class EnhancedCADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer, is_train=True):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Define augmentation pipeline\n",
    "        self.transform = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            # Add more augmentations as needed\n",
    "        ]) if is_train else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        \n",
    "        # Apply augmentation only during training\n",
    "        if self.is_train and self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        pixel_values = self.processor(\n",
    "            images=image, \n",
    "            return_tensors=\"pt\",\n",
    "            size={\"height\": IMAGE_SIZE, \"width\": IMAGE_SIZE}\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Enhanced Model Architecture\n",
    "class EnhancedVisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Freeze early layers of vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for block in self.vision_encoder.encoder.layer[-4:]:  # Unfreeze last 4 blocks\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        # Enhanced decoder with cross-attention\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        \n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2-medium\" if torch.cuda.is_available() else \"gpt2\",  # Larger model if possible\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Multi-layer connector with residual connection\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        self.connector = nn.Sequential(\n",
    "            nn.Linear(encoder_hidden_size, decoder_hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(decoder_hidden_size, decoder_hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
    "        )\n",
    "        self.residual = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        image_embeds = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Enhanced connector with residual\n",
    "        base_features = self.residual(image_embeds)\n",
    "        transformed_features = self.connector(image_embeds)\n",
    "        image_embeds = base_features + transformed_features\n",
    "        \n",
    "        # Decoder outputs\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=image_embeds.unsqueeze(1),\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return outputs.logits\n",
    "\n",
    "# Create enhanced datasets and dataloaders\n",
    "print(\"Creating enhanced datasets...\")\n",
    "train_dataset = EnhancedCADDataset(train_ds, processor, tokenizer, is_train=True)\n",
    "test_dataset = EnhancedCADDataset(test_ds, processor, tokenizer, is_train=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Initialize enhanced model\n",
    "model = EnhancedVisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.vision_encoder.parameters(), 'lr': LR/10},\n",
    "    {'params': model.code_decoder.parameters(), 'lr': LR},\n",
    "    {'params': model.connector.parameters(), 'lr': LR},\n",
    "    {'params': model.residual.parameters(), 'lr': LR}\n",
    "])\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "scaler = amp.GradScaler(enabled=DEVICE==\"cuda\")\n",
    "\n",
    "# Enhanced training loop\n",
    "print(\"Starting enhanced training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        with amp.autocast(enabled=DEVICE==\"cuda\"):\n",
    "            outputs = model(pixel_values, input_ids, attention_mask)\n",
    "            shift_logits = outputs[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "\n",
    "# Save enhanced model\n",
    "torch.save(model.state_dict(), \"enhanced_model.pth\")\n",
    "print(\"Enhanced training complete. Model saved.\")\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def enhanced_evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating enhanced predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # Get image embeddings\n",
    "            with amp.autocast(enabled=DEVICE==\"cuda\"):\n",
    "                vision_output = model.vision_encoder(pixel_values)\n",
    "                image_embeds = vision_output.last_hidden_state[:, 0, :]\n",
    "                base_features = model.residual(image_embeds)\n",
    "                transformed_features = model.connector(image_embeds)\n",
    "                image_embeds = base_features + transformed_features\n",
    "            \n",
    "            # Enhanced generation with beam search\n",
    "            output_ids = model.code_decoder.generate(\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=image_embeds.unsqueeze(1),\n",
    "                num_beams=BEAM_SIZE,\n",
    "                early_stopping=True,\n",
    "                temperature=0.7,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "            \n",
    "            # Decode generated code\n",
    "            generated_code = tokenizer.batch_decode(\n",
    "                output_ids, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            for i, code in enumerate(generated_code):\n",
    "                sample_id = idx * BATCH_SIZE + i\n",
    "                generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                \n",
    "                dataset_idx = idx * BATCH_SIZE + i\n",
    "                if dataset_idx < len(dataloader.dataset):\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[dataset_idx][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating enhanced syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # Evaluate IOU\n",
    "    print(\"Evaluating enhanced IOU...\")\n",
    "    iou_scores = []\n",
    "    eval_subset = references[:50]\n",
    "    \n",
    "    for ref in tqdm(eval_subset, desc=\"Calculating IOU\"):\n",
    "        try:\n",
    "            iou = get_iou_best(ref[\"generated\"], ref[\"reference\"])\n",
    "            iou_scores.append(iou)\n",
    "        except Exception as e:\n",
    "            print(f\"IOU failed for sample {ref['id']}: {str(e)}\")\n",
    "            iou_scores.append(0.0)\n",
    "    \n",
    "    avg_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# Evaluate enhanced model\n",
    "print(\"Evaluating enhanced model...\")\n",
    "syntax_rate, avg_iou = enhanced_evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Enhanced Model Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "print(f\"- Average IOU (50 samples): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecagent-technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
