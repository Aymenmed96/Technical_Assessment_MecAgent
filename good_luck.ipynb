{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Syntax Rate: 1.0\n",
      "IOU: 0.5834943417057687\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 4\n",
      "Loading dataset...\n",
      "Train samples: 200, Test samples: 50\n",
      "Creating datasets and dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 50/50 [04:32<00:00,  5.45s/it, loss=0.7716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:   2%|▏         | 1/50 [00:10<08:53, 10.89s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:   4%|▍         | 2/50 [00:23<09:26, 11.79s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:   6%|▌         | 3/50 [00:34<09:09, 11.69s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:   8%|▊         | 4/50 [00:45<08:46, 11.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  10%|█         | 5/50 [00:57<08:41, 11.59s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  12%|█▏        | 6/50 [01:08<08:13, 11.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  14%|█▍        | 7/50 [01:18<07:42, 10.76s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  16%|█▌        | 8/50 [01:27<07:17, 10.41s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  18%|█▊        | 9/50 [01:37<07:01, 10.28s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  20%|██        | 10/50 [01:47<06:43, 10.10s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  22%|██▏       | 11/50 [01:58<06:45, 10.39s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  24%|██▍       | 12/50 [02:09<06:42, 10.59s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  26%|██▌       | 13/50 [02:20<06:34, 10.66s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  28%|██▊       | 14/50 [02:30<06:16, 10.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  30%|███       | 15/50 [02:40<05:59, 10.28s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  32%|███▏      | 16/50 [02:50<05:45, 10.16s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  34%|███▍      | 17/50 [02:59<05:30, 10.00s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  36%|███▌      | 18/50 [03:09<05:13,  9.80s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  38%|███▊      | 19/50 [03:18<05:00,  9.70s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  40%|████      | 20/50 [03:28<04:49,  9.66s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  42%|████▏     | 21/50 [03:37<04:38,  9.60s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  44%|████▍     | 22/50 [03:46<04:27,  9.54s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  46%|████▌     | 23/50 [03:56<04:18,  9.56s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  48%|████▊     | 24/50 [04:06<04:07,  9.52s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  50%|█████     | 25/50 [04:15<03:57,  9.48s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  52%|█████▏    | 26/50 [04:24<03:46,  9.46s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  54%|█████▍    | 27/50 [04:34<03:36,  9.43s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  56%|█████▌    | 28/50 [04:43<03:25,  9.36s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  58%|█████▊    | 29/50 [04:53<03:18,  9.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  60%|██████    | 30/50 [05:05<03:24, 10.23s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  62%|██████▏   | 31/50 [05:19<03:37, 11.42s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  64%|██████▍   | 32/50 [05:33<03:42, 12.35s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  66%|██████▌   | 33/50 [05:48<03:40, 12.96s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  68%|██████▊   | 34/50 [06:02<03:35, 13.49s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  70%|███████   | 35/50 [06:17<03:27, 13.86s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  72%|███████▏  | 36/50 [06:32<03:16, 14.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  74%|███████▍  | 37/50 [06:42<02:48, 12.97s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  76%|███████▌  | 38/50 [06:57<02:42, 13.51s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  78%|███████▊  | 39/50 [07:11<02:30, 13.68s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  80%|████████  | 40/50 [07:26<02:20, 14.02s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  82%|████████▏ | 41/50 [07:42<02:11, 14.61s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  84%|████████▍ | 42/50 [07:52<01:46, 13.37s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  86%|████████▌ | 43/50 [08:02<01:25, 12.21s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  88%|████████▊ | 44/50 [08:11<01:08, 11.45s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  90%|█████████ | 45/50 [08:21<00:54, 10.87s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  92%|█████████▏| 46/50 [08:30<00:41, 10.31s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  94%|█████████▍| 47/50 [08:39<00:29,  9.95s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  96%|█████████▌| 48/50 [08:50<00:20, 10.29s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions:  98%|█████████▊| 49/50 [09:00<00:10, 10.24s/it]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=64) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Generating predictions: 100%|██████████| 50/50 [09:09<00:00, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating syntax rate...\n",
      "\n",
      "--- Sample Generations (first 5) ---\n",
      "\n",
      "Sample 1 Reference:\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(0.0, -0.75, -0.75), cq.Vector(3.749399456654644e-33, 1.0, -6.123233995736766e-17), cq.Vector(1.0, 0.0, 6.123233995736766e-17)))\n",
      "loop0=wp_sketch0.moveTo(1.5, 0.0).lineTo(1.5, 1.5).lineTo(0.0, 1.5).lineTo(0.0, 0.0).close()\n",
      "loop1=wp_sketch0.moveTo(0.7578947368421053, 0.5368421052631579).circle(0.14210526315789473)\n",
      "loop2=wp_sketch0.moveTo(0.7578947368421053, 0.9315789473684211).circle(0.14210526315789473)\n",
      "solid0=wp_sketch0.add(loop0).add(loop1).add(loop2).extrude(0.03125)\n",
      "solid=solid0\n",
      "\n",
      "\n",
      "Sample 1 Generated:\n",
      "\n",
      "The following is a workplane for sketch 0.\n",
      "\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.0, 0.0, 0.0), cq.Vector(\n",
      "--------------------\n",
      "\n",
      "Sample 2 Reference:\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.5625, 0.0, -0.1875), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop0=wp_sketch0.moveTo(0.0, 0.375).threePointArc((-0.1875, 0.1875), (0.0, 0.0)).close()\n",
      "solid0=wp_sketch0.add(loop0).extrude(0.125)\n",
      "solid=solid0\n",
      "# Generating a workplane for sketch 1\n",
      "wp_sketch1 = cq.Workplane(cq.Plane(cq.Vector(0.5625, 0.0, -0.1875), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop1=wp_sketch1.moveTo(0.0, 0.0).threePointArc((0.1875, 0.1875), (0.0, 0.375)).lineTo(0.0, 0.0).close()\n",
      "solid1=wp_sketch1.add(loop1).extrude(0.125)\n",
      "solid=solid.union(solid1)\n",
      "# Generating a workplane for sketch 2\n",
      "wp_sketch2 = cq.Workplane(cq.Plane(cq.Vector(-0.5625, 0.0, -0.1875), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop2=wp_sketch2.moveTo(1.125, 0.0).threePointArc((0.9355263157894738, 0.18947368421052632), (1.125, 0.37894736842105264)).lineTo(0.0, 0.37894736842105264).threePointArc((0.18947368421052632, 0.18947368421052632), (0.0, 0.0)).close()\n",
      "solid2=wp_sketch2.add(loop2).extrude(0.125)\n",
      "solid=solid.union(solid2)\n",
      "# Generating a workplane for sketch 3\n",
      "wp_sketch3 = cq.Workplane(cq.Plane(cq.Vector(-0.5625, 0.0, -0.1875), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop3=wp_sketch3.moveTo(0.0, 0.0).threePointArc((0.1875, 0.1875), (0.0, 0.375)).lineTo(0.0, 0.0).close()\n",
      "solid3=wp_sketch3.add(loop3).extrude(0.125)\n",
      "solid=solid.union(solid3)\n",
      "# Generating a workplane for sketch 4\n",
      "wp_sketch4 = cq.Workplane(cq.Plane(cq.Vector(0.5625, 0.0, -0.1875), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop4=wp_sketch4.moveTo(0.0, 0.375).threePointArc((-0.1875, 0.1875), (0.0, 0.0)).close()\n",
      "solid4=wp_sketch4.add(loop4).extrude(0.125)\n",
      "solid=solid.union(solid4)\n",
      "# Generating a workplane for sketch 5\n",
      "wp_sketch5 = cq.Workplane(cq.Plane(cq.Vector(-0.640625, -0.125, 0.0), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop5=wp_sketch5.moveTo(0.0734375, 0.0).circle(0.07500000000000001)\n",
      "solid5=wp_sketch5.add(loop5).extrude(-0.25)\n",
      "solid=solid.cut(solid5)\n",
      "# Generating a workplane for sketch 6\n",
      "wp_sketch6 = cq.Workplane(cq.Plane(cq.Vector(0.484375, -0.125, 0.0), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop6=wp_sketch6.moveTo(0.07500000000000001, 0.0).circle(0.07500000000000001)\n",
      "solid6=wp_sketch6.add(loop6).extrude(-0.25)\n",
      "solid=solid.cut(solid6)\n",
      "\n",
      "\n",
      "Sample 2 Generated:\n",
      "\n",
      "The following is a workplane for sketch 0.\n",
      "\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.0, 0.0, 0.0), cq.Vector(\n",
      "--------------------\n",
      "\n",
      "Sample 3 Reference:\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.578125, 0.0, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop0=wp_sketch0.moveTo(0.5842105263157895, 0.0).circle(0.5842105263157895)\n",
      "loop1=wp_sketch0.moveTo(0.0, 0.0).threePointArc((0.17511815627328084, -0.29900544272020846), (0.48684210526315785, -0.45032894736842105)).lineTo(0.48684210526315785, -0.09736842105263158).lineTo(0.12171052631578946, -0.09736842105263158).close()\n",
      "loop2=wp_sketch0.moveTo(0.5598684210526316, 0.29210526315789476).lineTo(0.5598684210526316, 0.24342105263157893).lineTo(0.5963815789473684, 0.24342105263157893).lineTo(0.5963815789473684, 0.29210526315789476).lineTo(0.6450657894736842, 0.29210526315789476).lineTo(0.6450657894736842, 0.5233552631578947).lineTo(0.5111842105263158, 0.5233552631578947).lineTo(0.5111842105263158, 0.29210526315789476).close()\n",
      "loop3=wp_sketch0.moveTo(0.5111842105263158, 0.29210526315789476).threePointArc((0.7137890668764648, 0.017989379338933742), (1.0345394736842104, -0.09736842105263158)).lineTo(0.669407894736842, -0.09736842105263158).lineTo(0.669407894736842, -0.45032894736842105).close()\n",
      "solid0=wp_sketch0.add(loop0).add(loop1).add(loop2).add(loop3).extrude(0.046875)\n",
      "solid=solid0\n",
      "# Generating a workplane for sketch 1\n",
      "wp_sketch1 = cq.Workplane(cq.Plane(cq.Vector(-0.1015625, 0.25, 0.046875), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop4=wp_sketch1.moveTo(0.03848684210526316, 0.0).lineTo(0.03848684210526316, -0.044901315789473685).lineTo(0.16036184210526316, -0.044901315789473685).lineTo(0.16036184210526316, 0.0).lineTo(0.19884868421052632, 0.0).lineTo(0.19884868421052632, 0.3046875).lineTo(0.0, 0.3046875).lineTo(0.0, 0.0).close()\n",
      "loop5=wp_sketch1.moveTo(0.07697368421052632, 0.03848684210526316).lineTo(0.07697368421052632, -0.00962171052631579).lineTo(0.121875, -0.00962171052631579).lineTo(0.121875, 0.03848684210526316).lineTo(0.16036184210526316, 0.03848684210526316).lineTo(0.16036184210526316, 0.26620065789473685).lineTo(0.03848684210526316, 0.26620065789473685).lineTo(0.03848684210526316, 0.03848684210526316).close()\n",
      "solid1=wp_sketch1.add(loop4).add(loop5).extrude(0.046875)\n",
      "solid=solid.union(solid1)\n",
      "\n",
      "\n",
      "Sample 3 Generated:\n",
      "\n",
      "The following is a workplane for sketch 0.\n",
      "\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.0, 0.0, 0.0), cq.Vector(\n",
      "--------------------\n",
      "\n",
      "Sample 4 Reference:\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.75, -0.640625, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop0=wp_sketch0.moveTo(0.0, 0.0).threePointArc((0.03237240839517088, -0.07815390739430259), (0.1105263157894737, -0.1105263157894737)).lineTo(1.3894736842105264, -0.1105263157894737).threePointArc((1.4676275916048291, -0.07815390739430259), (1.5, 0.0)).lineTo(1.5, 1.2789473684210526).threePointArc((1.4676275916048291, 1.3571012758153556), (1.3894736842105264, 1.3894736842105264)).lineTo(0.1105263157894737, 1.3894736842105264).threePointArc((0.03237240839517088, 1.3571012758153556), (0.0, 1.2789473684210526)).lineTo(0.0, 0.0).close()\n",
      "loop1=wp_sketch0.moveTo(0.7578947368421053, 0.631578947368421).circle(0.18947368421052632)\n",
      "solid0=wp_sketch0.add(loop0).add(loop1).extrude(0.2265625)\n",
      "solid=solid0\n",
      "# Generating a workplane for sketch 1\n",
      "wp_sketch1 = cq.Workplane(cq.Plane(cq.Vector(-0.375, 0.0, 0.2265625), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop2=wp_sketch1.moveTo(0.37894736842105264, 0.0).circle(0.37894736842105264)\n",
      "loop3=wp_sketch1.moveTo(0.37894736842105264, 0.0).circle(0.18947368421052632)\n",
      "solid1=wp_sketch1.add(loop2).add(loop3).extrude(-0.109375)\n",
      "solid=solid.cut(solid1)\n",
      "\n",
      "\n",
      "Sample 4 Generated:\n",
      "\n",
      "The following is a workplane for sketch 0.\n",
      "\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.0, 0.0, 0.0), cq.Vector(\n",
      "--------------------\n",
      "\n",
      "Sample 5 Reference:\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.375, -0.375, 0.0), cq.Vector(1.0, 0.0, 0.0), cq.Vector(0.0, 0.0, 1.0)))\n",
      "loop0=wp_sketch0.moveTo(0.75, 0.0).lineTo(0.75, 0.75).lineTo(0.0, 0.75).lineTo(0.0, 0.0).close()\n",
      "loop1=wp_sketch0.moveTo(0.37894736842105264, 0.37894736842105264).circle(0.13421052631578947)\n",
      "solid0=wp_sketch0.add(loop0).add(loop1).extrude(0.75)\n",
      "solid=solid0\n",
      "\n",
      "\n",
      "Sample 5 Generated:\n",
      "\n",
      "The following is a workplane for sketch 0.\n",
      "\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(-0.0, 0.0, 0.0), cq.Vector(\n",
      "--------------------\n",
      "\n",
      "==================================================\n",
      "Baseline Evaluation Results:\n",
      "- Valid Syntax Rate: 0.0000\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import ast # Using ast to check for valid syntax\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Reduce batch size for local/CPU execution to prevent memory issues\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "# Use a very small subset for quick demonstration\n",
    "SUBSET_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "# Using a smaller configuration for faster download and processing\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# --- Initialize Tokenizer and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set PAD token to EOS token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# IMPORTANT: Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        # Ensure image is resized correctly during processing\n",
    "        pixel_values = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Model Architecture (Corrected) ---\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "        # Load decoder with cross-attention support\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings if new tokens were added (like pad_token)\n",
    "        self.code_decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Connector to map vision encoder's hidden size to decoder's hidden size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        \n",
    "        # *** FIX: Use the entire sequence of patch embeddings, not just [CLS] ***\n",
    "        # The output shape is (batch_size, sequence_length, hidden_size)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Project encoder embeddings to match decoder's dimensions\n",
    "        encoder_hidden_states = self.connector(encoder_hidden_states)\n",
    "        \n",
    "        # Create an attention mask for the encoder's output to be used in cross-attention\n",
    "        # This tells the decoder to attend to all image patches.\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "        # Decoder outputs with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # This is the decoder's self-attention mask\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask, # This is for cross-attention\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloaders ---\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# --- Training Setup ---\n",
    "model = VisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# Use ignore_index for the padding token ID\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels for autoregressive training\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation Function (Corrected) ---\n",
    "def evaluate_syntax_rate_simple(generated_codes):\n",
    "    \"\"\"\n",
    "    A simple syntax checker using Python's `ast` module.\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    total_count = len(generated_codes)\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for code in generated_codes.values():\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            valid_count += 1\n",
    "        except (SyntaxError, ValueError):\n",
    "            continue\n",
    "    return valid_count / total_count\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # *** FIX: Generate using the full encoder output as context ***\n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            \n",
    "            # Create an attention mask for the encoder's output, similar to what's done in forward\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "            # Create a dummy input_ids for the decoder to start generating\n",
    "            # The shape is (batch_size, 1) and it contains the EOS token as a BOS token.\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.eos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_new_tokens=64,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                num_beams=1,\n",
    "                do_sample=False,  \n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode generated code\n",
    "            batch_generated_code = tokenizer.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            for i, code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # The IOU calculation is computationally expensive and requires a specific\n",
    "    # environment to run CadQuery scripts. We will print the generated\n",
    "    # code for manual inspection instead.\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nSample {i+1} Reference:\")\n",
    "        print(references[i]['reference'])\n",
    "        print(f\"\\nSample {i+1} Generated:\")\n",
    "        print(references[i]['generated'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Returning a dummy IOU value as we can't compute it here.\n",
    "    avg_iou = 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "syntax_rate, avg_iou = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "# print(f\"- Average IOU (dummy value): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Enhancements and Rationale:\n",
    "### 1- Model Architecture Improvements:\n",
    "\n",
    "    Partial Vision Encoder Freezing: Freeze early layers of ViT to prevent overfitting while allowing later layers to adapt to CAD features\n",
    "\n",
    "    Smaller Decoder: Reduced GPT-2 layers from 12 to 6 for efficiency\n",
    "\n",
    "    Enhanced Connector: Added residual connection and layer normalization for better gradient flow\n",
    "\n",
    "    Domain-Specific Tokens: Added CAD operation tokens to vocabulary\n",
    "\n",
    "### 2-Regularization Techniques:\n",
    "\n",
    "    Dropout: Added dropout in connector and decoder (0.2 rate)\n",
    "\n",
    "    Weight Decay: L2 regularization in optimizer (0.01)\n",
    "\n",
    "    Data Augmentation: Random horizontal flips during training\n",
    "\n",
    "    Gradient Clipping: Prevents exploding gradients (max norm=1.0)\n",
    "\n",
    "### 3-Training Optimization:\n",
    "\n",
    "    Learning Rate Warmup: Gradual LR increase for first 50 steps\n",
    "\n",
    "    Learning Rate Scheduling: ReduceLROnPlateau monitors validation loss\n",
    "\n",
    "    Parallel Data Loading: num_workers=2 for faster data loading\n",
    "\n",
    "    Epoch Loss Tracking: Better monitoring of training progress\n",
    "\n",
    "### 4-Evaluation Enhancements:\n",
    "\n",
    "    Validation Loss: Added proper validation loss calculation\n",
    "\n",
    "    Diverse Beam Search: num_beam_groups=3 with diversity penalty\n",
    "\n",
    "    Enhanced Generation: Larger beam width (6 beams) for better results\n",
    "\n",
    "## Potential Bottlenecks and Solutions:\n",
    "### 1-Memory Constraints:\n",
    "\n",
    "    Bottleneck: Larger models/batches may exceed GPU memory\n",
    "\n",
    "    Mitigation: Used smaller decoder, gradient clipping\n",
    "\n",
    "### 2-Overfitting:\n",
    "\n",
    "    Bottleneck: Small dataset (200 samples) risks overfitting\n",
    "\n",
    "    Mitigation: Dropout, weight decay, partial freezing, data augmentation\n",
    "\n",
    "### 3-Model Capacity:\n",
    "\n",
    "    Bottleneck: Reduced decoder size may limit expressiveness\n",
    "\n",
    "    Mitigation: Enhanced connector with residual connections\n",
    "\n",
    "### 4-Evaluation Limitations:\n",
    "\n",
    "    Bottleneck: No geometric evaluation (IOU)\n",
    "\n",
    "    Mitigation: Added validation loss as proxy metric\n",
    "\n",
    "### 5-Training Stability:\n",
    "\n",
    "    Bottleneck: Fluctuating loss with small batches\n",
    "\n",
    "    Mitigation: Gradient clipping, LR warmup, and scheduling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecagent-technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
