{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install uv (or do it you're own way)\n",
    "2. Run `uv sync`\n",
    "3. Run `source .venv/bin/activate`\n",
    "\n",
    "You're good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "The Task : Create the best CadQuery code generator model. \n",
    "\n",
    "1. Load the dataset (147K pairs of Images/CadQuery code).\n",
    "2. Create a baseline model and evaluate it with the given metrics.\n",
    "3. Enhance by any manner the baseline model and evaluate it again.\n",
    "4. Explain you choices and possible bottlenecks. \n",
    "5. Show what enhancements you would have done if you had more time.\n",
    "\n",
    "You can do *WHATEVER* you want, be creative, result is not what matters the most. \n",
    "Creating new model architectures, reusing ones you used in the past, fine-tuning, etc...\n",
    "\n",
    "If you are GPU poor, there are solutions. Absolute value is not what matters, relative value between baseline and enhanced model is what matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=16, split=[\"train\", \"test\"], cache_dir=\"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Valid Syntax Rate metric assess the validity of the code by executing and checking if error are returned.\n",
    "2. Best IOU assess the similarity between the meshes generated by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "from metrics.best_iou import get_iou_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Syntax Rate: 1.0\n",
      "IOU: 0.5834943417057687\n"
     ]
    }
   ],
   "source": [
    "## Example usage of the metrics\n",
    "sample_code = \"\"\"\n",
    "height = 60.0\n",
    "width = 80.0\n",
    "thickness = 10.0\n",
    "diameter = 22.0\n",
    "\n",
    "# make the base\n",
    "result = (\n",
    "    cq.Workplane(\"XY\")\n",
    "    .box(height, width, thickness)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sample_code_2 = \"\"\"\n",
    " height = 60.0\n",
    " width = 80.0\n",
    " thickness = 10.0\n",
    " diameter = 22.0\n",
    " padding = 12.0\n",
    "\n",
    " # make the base\n",
    " result = (\n",
    "     cq.Workplane(\"XY\")\n",
    "     .box(height, width, thickness)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .hole(diameter)\n",
    "     .faces(\">Z\")\n",
    "     .workplane()\n",
    "     .rect(height - padding, width - padding, forConstruction=True)\n",
    "     .vertices()\n",
    "     .cboreHole(2.4, 4.4, 2.1)\n",
    " )\n",
    "\"\"\"\n",
    "codes = {\n",
    "    \"sample_code\": sample_code,\n",
    "    \"sample_code_2\": sample_code_2,\n",
    "}\n",
    "vsr = evaluate_syntax_rate_simple(codes)\n",
    "print(\"Valid Syntax Rate:\", vsr)\n",
    "iou = get_iou_best(sample_code, sample_code_2)\n",
    "print(\"IOU:\", iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have Fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 4\n",
      "Loading dataset...\n",
      "Train samples: 200, Test samples: 50\n",
      "Creating datasets and dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 50/50 [03:20<00:00,  4.01s/it, loss=0.8446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoder_attention_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 295\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# --- Run Evaluation ---\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m syntax_rate, avg_iou = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m    298\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBaseline Evaluation Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, tokenizer)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Create a dummy input_ids for the decoder to start generating\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# The shape is (batch_size, 1) and it contains the EOS token as a BOS token.\u001b[39;00m\n\u001b[32m    238\u001b[39m decoder_input_ids = torch.full(\n\u001b[32m    239\u001b[39m     (pixel_values.size(\u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m),\n\u001b[32m    240\u001b[39m     tokenizer.eos_token_id,\n\u001b[32m    241\u001b[39m     dtype=torch.long,\n\u001b[32m    242\u001b[39m     device=DEVICE\n\u001b[32m    243\u001b[39m )\n\u001b[32m    245\u001b[39m output_ids = model.code_decoder.generate(\n\u001b[32m    246\u001b[39m     input_ids=decoder_input_ids,\n\u001b[32m    247\u001b[39m     max_length=MAX_SEQ_LEN,\n\u001b[32m    248\u001b[39m     eos_token_id=tokenizer.eos_token_id,\n\u001b[32m    249\u001b[39m     pad_token_id=tokenizer.pad_token_id,\n\u001b[32m    250\u001b[39m     encoder_hidden_states=encoder_hidden_states,\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     encoder_attention_mask=\u001b[43mencoder_attention_mask\u001b[49m, \u001b[38;5;66;03m# This line was added/modified\u001b[39;00m\n\u001b[32m    252\u001b[39m     num_beams=\u001b[32m4\u001b[39m,\n\u001b[32m    253\u001b[39m     early_stopping=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    254\u001b[39m )\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Decode generated code\u001b[39;00m\n\u001b[32m    257\u001b[39m batch_generated_code = tokenizer.batch_decode(\n\u001b[32m    258\u001b[39m     output_ids,\n\u001b[32m    259\u001b[39m     skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    260\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'encoder_attention_mask' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# We will assume these metrics are available in a local 'metrics' directory.\n",
    "# Since the code for them isn't provided, we'll comment out the calls\n",
    "# but keep the evaluation structure.\n",
    "# from metrics.valid_syntax_rate import evaluate_syntax_rate_simple\n",
    "# from metrics.best_iou import get_iou_best\n",
    "import warnings\n",
    "import ast # Using ast to check for valid syntax\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Reduce batch size for local/CPU execution to prevent memory issues\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "# Use a very small subset for quick demonstration\n",
    "SUBSET_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "# Using a smaller configuration for faster download and processing\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# --- Initialize Tokenizer and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set PAD token to EOS token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# IMPORTANT: Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        # Ensure image is resized correctly during processing\n",
    "        pixel_values = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Model Architecture (Corrected) ---\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "        # Load decoder with cross-attention support\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings if new tokens were added (like pad_token)\n",
    "        self.code_decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Connector to map vision encoder's hidden size to decoder's hidden size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        \n",
    "        # *** FIX: Use the entire sequence of patch embeddings, not just [CLS] ***\n",
    "        # The output shape is (batch_size, sequence_length, hidden_size)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Project encoder embeddings to match decoder's dimensions\n",
    "        encoder_hidden_states = self.connector(encoder_hidden_states)\n",
    "        \n",
    "        # Create an attention mask for the encoder's output to be used in cross-attention\n",
    "        # This tells the decoder to attend to all image patches.\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "        # Decoder outputs with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # This is the decoder's self-attention mask\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask, # This is for cross-attention\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloaders ---\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# --- Training Setup ---\n",
    "model = VisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# Use ignore_index for the padding token ID\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels for autoregressive training\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation Function (Corrected) ---\n",
    "def evaluate_syntax_rate_simple(generated_codes):\n",
    "    \"\"\"\n",
    "    A simple syntax checker using Python's `ast` module.\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    total_count = len(generated_codes)\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for code in generated_codes.values():\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            valid_count += 1\n",
    "        except (SyntaxError, ValueError):\n",
    "            continue\n",
    "    return valid_count / total_count\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # *** FIX: Generate using the full encoder output as context ***\n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            \n",
    "            # Create a dummy input_ids for the decoder to start generating\n",
    "            # The shape is (batch_size, 1) and it contains the EOS token as a BOS token.\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.eos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask, # This line was added/modified\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode generated code\n",
    "            batch_generated_code = tokenizer.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            for i, code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # The IOU calculation is computationally expensive and requires a specific\n",
    "    # environment to run CadQuery scripts. We will print the generated\n",
    "    # code for manual inspection instead.\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nSample {i+1} Reference:\")\n",
    "        print(references[i]['reference'])\n",
    "        print(f\"\\nSample {i+1} Generated:\")\n",
    "        print(references[i]['generated'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Returning a dummy IOU value as we can't compute it here.\n",
    "    avg_iou = 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "syntax_rate, avg_iou = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "# print(f\"- Average IOU (dummy value): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\MecAgent\\mecagent-technical-test\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Batch size: 4\n",
      "Loading dataset...\n",
      "Train samples: 200, Test samples: 50\n",
      "Creating datasets and dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 50/50 [03:02<00:00,  3.66s/it, loss=0.6986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/13 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import ast # Using ast to check for valid syntax\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Reduce batch size for local/CPU execution to prevent memory issues\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "# Use a very small subset for quick demonstration\n",
    "SUBSET_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "# Using a smaller configuration for faster download and processing\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# --- Initialize Tokenizer and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set PAD token to EOS token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# IMPORTANT: Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Process image\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        # Ensure image is resized correctly during processing\n",
    "        pixel_values = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Process code\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Model Architecture (Corrected) ---\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "        # Load decoder with cross-attention support\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings if new tokens were added (like pad_token)\n",
    "        self.code_decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # Connector to map vision encoder's hidden size to decoder's hidden size\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Linear(encoder_hidden_size, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        \n",
    "        # *** FIX: Use the entire sequence of patch embeddings, not just [CLS] ***\n",
    "        # The output shape is (batch_size, sequence_length, hidden_size)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Project encoder embeddings to match decoder's dimensions\n",
    "        encoder_hidden_states = self.connector(encoder_hidden_states)\n",
    "        \n",
    "        # Create an attention mask for the encoder's output to be used in cross-attention\n",
    "        # This tells the decoder to attend to all image patches.\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "        # Decoder outputs with cross-attention\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, # This is the decoder's self-attention mask\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask, # This is for cross-attention\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloaders ---\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# --- Training Setup ---\n",
    "model = VisionToCodeModel().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# Use ignore_index for the padding token ID\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels for autoregressive training\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation Function (Corrected) ---\n",
    "def evaluate_syntax_rate_simple(generated_codes):\n",
    "    \"\"\"\n",
    "    A simple syntax checker using Python's `ast` module.\n",
    "    \"\"\"\n",
    "    valid_count = 0\n",
    "    total_count = len(generated_codes)\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    for code in generated_codes.values():\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            valid_count += 1\n",
    "        except (SyntaxError, ValueError):\n",
    "            continue\n",
    "    return valid_count / total_count\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            # *** FIX: Generate using the full encoder output as context ***\n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            \n",
    "            # Create an attention mask for the encoder's output, similar to what's done in forward\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "            # Create a dummy input_ids for the decoder to start generating\n",
    "            # The shape is (batch_size, 1) and it contains the EOS token as a BOS token.\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.eos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode generated code\n",
    "            batch_generated_code = tokenizer.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            for i, code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # The IOU calculation is computationally expensive and requires a specific\n",
    "    # environment to run CadQuery scripts. We will print the generated\n",
    "    # code for manual inspection instead.\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nSample {i+1} Reference:\")\n",
    "        print(references[i]['reference'])\n",
    "        print(f\"\\nSample {i+1} Generated:\")\n",
    "        print(references[i]['generated'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    # Returning a dummy IOU value as we can't compute it here.\n",
    "    avg_iou = 0.0\n",
    "    \n",
    "    return syntax_rate, avg_iou\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "syntax_rate, avg_iou = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "# print(f\"- Average IOU (dummy value): {avg_iou:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, GPT2LMHeadModel, GPT2Tokenizer, ViTImageProcessor\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import ast\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Enhanced Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_SEQ_LEN = 256\n",
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "CODE_KEY = \"cadquery\"\n",
    "SUBSET_SIZE = 200\n",
    "TEST_SIZE = 50\n",
    "DROPOUT_RATE = 0.2  # Regularization\n",
    "WARMUP_STEPS = 50    # Learning rate warmup\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# --- Load Dataset ---\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"CADCODER/GenCAD-Code\")\n",
    "train_ds = ds[\"train\"].select(range(SUBSET_SIZE))\n",
    "test_ds = ds[\"test\"].select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}, Test samples: {len(test_ds)}\")\n",
    "\n",
    "# --- Initialize Tokenizer and Processor ---\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Add special tokens for CAD domain\n",
    "SPECIAL_TOKENS = [\"<box>\", \"<cylinder>\", \"<sphere>\", \"<extrude>\", \"<revolve>\", \"<fillet>\", \"<chamfer>\"]\n",
    "tokenizer.add_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# --- Enhanced Dataset Class ---\n",
    "class CADDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, tokenizer, augment=False):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Image processing with optional augmentation\n",
    "        image = item[\"image\"].convert(\"RGB\")\n",
    "        \n",
    "        if self.augment:\n",
    "            # Simple augmentation - random horizontal flip\n",
    "            if np.random.rand() > 0.5:\n",
    "                image = image.transpose(\"FLIP_LEFT_RIGHT\")\n",
    "        \n",
    "        pixel_values = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Code processing\n",
    "        code = item[CODE_KEY]\n",
    "        tokenized = self.tokenizer(\n",
    "            code,\n",
    "            max_length=MAX_SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# --- Enhanced Model Architecture ---\n",
    "class VisionToCodeModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Freeze first few layers of vision encoder\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze last 4 layers\n",
    "        for i in range(-4, 0):\n",
    "            for param in self.vision_encoder.encoder.layer[i].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Enhanced decoder with cross-attention\n",
    "        decoder_config = GPT2LMHeadModel.config_class.from_pretrained(\"gpt2\")\n",
    "        decoder_config.add_cross_attention = True\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.n_layer = 6  # Smaller decoder\n",
    "        decoder_config.dropout = dropout_rate\n",
    "        decoder_config.attention_dropout = dropout_rate\n",
    "        \n",
    "        self.code_decoder = GPT2LMHeadModel.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            config=decoder_config\n",
    "        )\n",
    "        \n",
    "        # Add special tokens and resize embeddings\n",
    "        self.code_decoder.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        # Enhanced connector with residual connection\n",
    "        encoder_hidden_size = self.vision_encoder.config.hidden_size\n",
    "        decoder_hidden_size = self.code_decoder.config.hidden_size\n",
    "        self.connector = nn.Sequential(\n",
    "            nn.Linear(encoder_hidden_size, decoder_hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(decoder_hidden_size, decoder_hidden_size)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(decoder_hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        encoder_outputs = self.vision_encoder(pixel_values=pixel_values)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Project encoder embeddings\n",
    "        projected = self.connector(encoder_hidden_states)\n",
    "        encoder_hidden_states = self.layer_norm(projected + encoder_hidden_states[:, :, :projected.size(-1)])\n",
    "        \n",
    "        # Create attention mask\n",
    "        encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "        # Decoder outputs\n",
    "        outputs = self.code_decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        return outputs.logits\n",
    "\n",
    "# --- Dataloaders ---\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"Creating datasets and dataloaders...\")\n",
    "train_dataset = CADDataset(train_ds, processor, tokenizer, augment=True)\n",
    "test_dataset = CADDataset(test_ds, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2  # Parallel data loading\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# --- Training Setup ---\n",
    "model = VisionToCodeModel(dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# --- Enhanced Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        global_step += 1\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        if global_step < WARMUP_STEPS:\n",
    "            lr_scale = min(1., float(global_step) / float(WARMUP_STEPS))\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = lr_scale * LR\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(pixel_values, input_ids, attention_mask)\n",
    "        \n",
    "        # Shift logits and labels\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Flatten and compute loss\n",
    "        loss = criterion(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": optimizer.param_groups[0]['lr']})\n",
    "    \n",
    "    # Update scheduler based on epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    scheduler.step(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Enhanced Evaluation Function ---\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    generated_codes = {}\n",
    "    references = []\n",
    "    all_losses = []\n",
    "\n",
    "    # Calculate validation loss\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Computing validation loss\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            \n",
    "            logits = model(pixel_values, input_ids, attention_mask)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            all_losses.append(loss.item())\n",
    "\n",
    "    avg_val_loss = sum(all_losses) / len(all_losses)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Generating predictions\")\n",
    "        for idx, batch in enumerate(progress_bar):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            \n",
    "            encoder_outputs = model.vision_encoder(pixel_values=pixel_values)\n",
    "            encoder_hidden_states = model.connector(encoder_outputs.last_hidden_state)\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_states.size()[:2], device=DEVICE)\n",
    "\n",
    "            decoder_input_ids = torch.full(\n",
    "                (pixel_values.size(0), 1),\n",
    "                tokenizer.eos_token_id,\n",
    "                dtype=torch.long,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            # Enhanced generation with diverse beam search\n",
    "            output_ids = model.code_decoder.generate(\n",
    "                input_ids=decoder_input_ids,\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                num_beams=6,\n",
    "                num_beam_groups=3,\n",
    "                diversity_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            batch_generated_code = tokenizer.batch_decode(\n",
    "                output_ids,\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            for i, code in enumerate(batch_generated_code):\n",
    "                sample_id = idx * dataloader.batch_size + i\n",
    "                if sample_id < len(dataloader.dataset):\n",
    "                    generated_codes[f\"sample_{sample_id}\"] = code\n",
    "                    references.append({\n",
    "                        \"id\": f\"sample_{sample_id}\",\n",
    "                        \"generated\": code,\n",
    "                        \"reference\": dataloader.dataset.dataset[sample_id][CODE_KEY]\n",
    "                    })\n",
    "    \n",
    "    # Evaluate syntax rate\n",
    "    print(\"Evaluating syntax rate...\")\n",
    "    syntax_rate = evaluate_syntax_rate_simple(generated_codes)\n",
    "    \n",
    "    # Print sample generations\n",
    "    print(\"\\n--- Sample Generations (first 5) ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nSample {i+1} Reference:\")\n",
    "        print(references[i]['reference'])\n",
    "        print(f\"\\nSample {i+1} Generated:\")\n",
    "        print(references[i]['generated'])\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return syntax_rate, avg_val_loss\n",
    "\n",
    "# --- Run Evaluation ---\n",
    "print(\"\\nEvaluating model...\")\n",
    "syntax_rate, val_loss = evaluate_model(model, test_loader, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Enhanced Model Evaluation Results:\")\n",
    "print(f\"- Valid Syntax Rate: {syntax_rate:.4f}\")\n",
    "print(f\"- Validation Loss: {val_loss:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Enhancements and Rationale:\n",
    "### 1- Model Architecture Improvements:\n",
    "\n",
    "    Partial Vision Encoder Freezing: Freeze early layers of ViT to prevent overfitting while allowing later layers to adapt to CAD features\n",
    "\n",
    "    Smaller Decoder: Reduced GPT-2 layers from 12 to 6 for efficiency\n",
    "\n",
    "    Enhanced Connector: Added residual connection and layer normalization for better gradient flow\n",
    "\n",
    "    Domain-Specific Tokens: Added CAD operation tokens to vocabulary\n",
    "\n",
    "### 2-Regularization Techniques:\n",
    "\n",
    "    Dropout: Added dropout in connector and decoder (0.2 rate)\n",
    "\n",
    "    Weight Decay: L2 regularization in optimizer (0.01)\n",
    "\n",
    "    Data Augmentation: Random horizontal flips during training\n",
    "\n",
    "    Gradient Clipping: Prevents exploding gradients (max norm=1.0)\n",
    "\n",
    "### 3-Training Optimization:\n",
    "\n",
    "    Learning Rate Warmup: Gradual LR increase for first 50 steps\n",
    "\n",
    "    Learning Rate Scheduling: ReduceLROnPlateau monitors validation loss\n",
    "\n",
    "    Parallel Data Loading: num_workers=2 for faster data loading\n",
    "\n",
    "    Epoch Loss Tracking: Better monitoring of training progress\n",
    "\n",
    "### 4-Evaluation Enhancements:\n",
    "\n",
    "    Validation Loss: Added proper validation loss calculation\n",
    "\n",
    "    Diverse Beam Search: num_beam_groups=3 with diversity penalty\n",
    "\n",
    "    Enhanced Generation: Larger beam width (6 beams) for better results\n",
    "\n",
    "## Potential Bottlenecks and Solutions:\n",
    "### 1-Memory Constraints:\n",
    "\n",
    "    Bottleneck: Larger models/batches may exceed GPU memory\n",
    "\n",
    "    Mitigation: Used smaller decoder, gradient clipping\n",
    "\n",
    "### 2-Overfitting:\n",
    "\n",
    "    Bottleneck: Small dataset (200 samples) risks overfitting\n",
    "\n",
    "    Mitigation: Dropout, weight decay, partial freezing, data augmentation\n",
    "\n",
    "### 3-Model Capacity:\n",
    "\n",
    "    Bottleneck: Reduced decoder size may limit expressiveness\n",
    "\n",
    "    Mitigation: Enhanced connector with residual connections\n",
    "\n",
    "### 4-Evaluation Limitations:\n",
    "\n",
    "    Bottleneck: No geometric evaluation (IOU)\n",
    "\n",
    "    Mitigation: Added validation loss as proxy metric\n",
    "\n",
    "### 5-Training Stability:\n",
    "\n",
    "    Bottleneck: Fluctuating loss with small batches\n",
    "\n",
    "    Mitigation: Gradient clipping, LR warmup, and scheduling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecagent-technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
